{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    TensorDataset,\n",
    "    Dataset\n",
    ")\n",
    "from torch.optim import (Optimizer, Adam)\n",
    "from torch.optim import SGD\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f6ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Titanic-Dataset.csv\"\n",
    "\n",
    "# Get data in pandas dataframe\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[['Survived','Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']]\n",
    "    \n",
    "    df = df.replace(\"male\", 0)\n",
    "    df = df.replace(\"female\", 1)\n",
    "    # Replace NaN\n",
    "    df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "    \n",
    "    # Get rid of blanks, get only cabin class - shows upper to lower decks, \n",
    "    # then 1-hot encode (seperate each class to binary columns)\n",
    "    # Will end up with around 8 binary columns\n",
    "    # Cabin_U = unknown\n",
    "    df['Cabin'] = df['Cabin'].fillna('Unknown')\n",
    "    df['Cabin'] = df['Cabin'].str[0]\n",
    "    df = pd.get_dummies(df, columns=['Cabin'])\n",
    "\n",
    "    # 1-hot encoding for 3 binary columns repping port of embarkment\n",
    "    df = pd.get_dummies(df, columns=['Embarked'])\n",
    "\n",
    "    bool_cols = df.select_dtypes(include='bool').columns\n",
    "    df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb634d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Survived  Pclass  Sex        Age  SibSp  Parch     Fare  Cabin_A  \\\n",
      "0           0       3    0  22.000000      1      0   7.2500        0   \n",
      "1           1       1    1  38.000000      1      0  71.2833        0   \n",
      "2           1       3    1  26.000000      0      0   7.9250        0   \n",
      "3           1       1    1  35.000000      1      0  53.1000        0   \n",
      "4           0       3    0  35.000000      0      0   8.0500        0   \n",
      "..        ...     ...  ...        ...    ...    ...      ...      ...   \n",
      "886         0       2    0  27.000000      0      0  13.0000        0   \n",
      "887         1       1    1  19.000000      0      0  30.0000        0   \n",
      "888         0       3    1  29.699118      1      2  23.4500        0   \n",
      "889         1       1    0  26.000000      0      0  30.0000        0   \n",
      "890         0       3    0  32.000000      0      0   7.7500        0   \n",
      "\n",
      "     Cabin_B  Cabin_C  Cabin_D  Cabin_E  Cabin_F  Cabin_G  Cabin_T  Cabin_U  \\\n",
      "0          0        0        0        0        0        0        0        1   \n",
      "1          0        1        0        0        0        0        0        0   \n",
      "2          0        0        0        0        0        0        0        1   \n",
      "3          0        1        0        0        0        0        0        0   \n",
      "4          0        0        0        0        0        0        0        1   \n",
      "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "886        0        0        0        0        0        0        0        1   \n",
      "887        1        0        0        0        0        0        0        0   \n",
      "888        0        0        0        0        0        0        0        1   \n",
      "889        0        1        0        0        0        0        0        0   \n",
      "890        0        0        0        0        0        0        0        1   \n",
      "\n",
      "     Embarked_C  Embarked_Q  Embarked_S  \n",
      "0             0           0           1  \n",
      "1             1           0           0  \n",
      "2             0           0           1  \n",
      "3             0           0           1  \n",
      "4             0           0           1  \n",
      "..          ...         ...         ...  \n",
      "886           0           0           1  \n",
      "887           0           0           1  \n",
      "888           0           0           1  \n",
      "889           1           0           0  \n",
      "890           0           1           0  \n",
      "\n",
      "[891 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ollys\\AppData\\Local\\Temp\\ipykernel_12396\\3049212081.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(\"female\", 1)\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = clean_df(df)\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Torch dataset with tensors\n",
    "# Need to update this to split test data, ie make 10% of dataset test data, 90% is training\n",
    "def make_datset(df: pd.DataFrame) -> Dataset:\n",
    "    features = df.drop(columns=['Survived'])\n",
    "    \n",
    "    # Ytrue\n",
    "    target = df['Survived']\n",
    "\n",
    "    x_values = torch.tensor(features.values, dtype=torch.float32)\n",
    "    y_values = torch.tensor(target.values, dtype=torch.int64)\n",
    "    return TensorDataset(x_values,y_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d66ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_datset(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b13c8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(dataset: Dataset, batch_size:int, shuffle:bool) -> DataLoader:\n",
    "    return DataLoader(dataset, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6f10d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = make_dataloader(train_dataset, shuffle=False, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad171f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to do linear, logistic, 2 types of MLP, maybe also compare custom vs built in\n",
    "# like custom gradient descent vs built in optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1cd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear classifier model\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.linear = nn.Linear(num_features, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        # Makes logit\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-layer MLP and 2-layer MLP\n",
    "# Build 1 using custom \n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        # Hidden layer: 4 features applied to 100 neurons do we\n",
    "        self.linear1 = nn.Linear(num_features, 100)\n",
    "        # act 1\n",
    "        self.act1 = nn.ReLU()\n",
    "        # Output layer - binary classification for titanic survival\n",
    "        self.output = nn.Linear(100, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer MLP\n",
    "class MLP2DClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.linear1 = nn.Linear(num_features, 100)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(100,50)\n",
    "        self.output = nn.Linear(50,2)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de10a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also do 4 layers with a residual network?\n",
    "# This will probably overfit, good for investigation for assignment\n",
    "# Also encoder counts as investigation even though it doesn't go with this data right\n",
    "# SOMEONE WE NEED TO FIGURE OUT SKIP CONNECTION HERE FOR RESIDUAL NETWORK\n",
    "class MLP4DResidualClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.linear1 = nn.Linear(num_features, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128,64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(64,32)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(32,16)\n",
    "        self.output = nn.Linear(16,2)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we want custom vs built in?\n",
    "# Can create cross entropy function and gradient descent and compare to built in mlp\n",
    "# We do this for whichever mlp is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf968e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model: nn.Module, loss: nn.Module, optimizer: Optimizer, dataloader: DataLoader, epochs: int):\n",
    "    for epoch in range(epochs):\n",
    "        for (x,target) in dataloader:\n",
    "            pred = model(x)\n",
    "            l = loss(pred, target)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Print every accuracy every 10 epochs, do we need to change how we do this?\n",
    "        if epoch % 10 == 0:\n",
    "            [w,b] = model.parameters()\n",
    "            print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "n_samples, n_features = train_dataset.tensors[0].shape\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0588ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training linear\n",
    "lin = LinearClassifier(n_features)\n",
    "loss = nn.MSELoss()\n",
    "# May want to adjust learning rate\n",
    "optimizer = SGD(lin.parameters(), lr=0.001)\n",
    "# May want less epochs\n",
    "train(lin, loss, optimizer, train_dataloader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359eafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Logistic\n",
    "log = LogisticRegression(n_features)\n",
    "loss = nn.BCELoss()\n",
    "# May want to adjust learning rate\n",
    "optimizer = SGD(log.parameters(), lr=0.001)\n",
    "train(log,loss,optimizer,train_dataloader,n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78345a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLP\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 1 layer\n",
    "mlp = MLPClassifier(n_features)\n",
    "optimizer = Adam(mlp.parameters(), lr=0.001)\n",
    "train(mlp,loss,optimizer,train_dataloader,n_epochs)\n",
    "\n",
    "# 2 layers\n",
    "mlp2 = MLP2DClassifier(n_features)\n",
    "optimizer = Adam(mlp2.parameters(), lr=0.001)\n",
    "train(mlp2,loss,optimizer,train_dataloader,n_epochs)\n",
    "\n",
    "# 4 layers residual - note residual not added yet\n",
    "mlp4 = MLP4DResidualClassifier(n_features)\n",
    "optimizer = Adam(mlp4.parameters(), lr=0.001)\n",
    "train(mlp4,loss,optimizer,train_dataloader,n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train all, whichever performs best we integrate into final game (system integration part 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
